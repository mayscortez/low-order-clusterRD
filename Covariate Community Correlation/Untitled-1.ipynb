{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation_n1000_nc50_in05_out00_p006_B006_deg1_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p025_B006_deg1_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p0333_B006_deg1_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p0667_B006_deg1_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p1_B006_deg1_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p006_B006_deg2_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p025_B006_deg2_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p0333_B006_deg2_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p0667_B006_deg2_bernoulli-full.csv done\n",
      "correlation_n1000_nc50_in05_out00_p1_B006_deg2_bernoulli-full.csv done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfile_name = 'correlation_n1000_nc50_in05_out00_p1_B002_deg1_bernoulli-full.csv'\\n\\ndf = pd.read_csv(file_path + file_name)\\n\\n# delete rows where (Estimator == LS-Prop or Estimator == LS-Num) and design == Cluster\\ndf_new = df.drop(df[((df['Estimator'] == 'LS-Prop') | (df['Estimator'] == 'LS-Num')) & (df['design'] == 'Cluster')].index)\\n\\n# where Estimator == DM and Design == Cluster, rename Estimator to DM-C\\ndf_new.loc[(df['Estimator']=='DM') & (df['design']=='Cluster'), 'Estimator'] = 'DM-C'\\n\\n# where Estimator == DM($0.75$) and Design == Cluster, rename Esimator to DM-C($0.75$)\\ndf_new.loc[(df['Estimator']=='DM($0.75$)') & (df['design']=='Cluster'), 'Estimator'] = 'DM-C($0.75$)'\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "file_path = 'output/deg1/bernoulli/'\n",
    "\n",
    "beta = [1,2]\n",
    "B = [0.06, 0.06] \n",
    "probs = [[0.06, 0.25, 1/3, 2/3, 1], [0.06, 0.25, 1/3, 2/3, 1]]\n",
    "design = \"bernoulli\"\n",
    "n = 1000                  \n",
    "nc = 50  \n",
    "p_in = 0.5                 \n",
    "p_out = (0.5-p_in)/49\n",
    "\n",
    "for b in range(len(beta)):\n",
    "    for p in probs[b]:\n",
    "        file_path = 'output/deg' + str(beta[b]) + '/bernoulli/'\n",
    "        fixed = '_n' + str(n) + '_nc' + str(nc) + '_' + 'in' + str(np.round(p_in,3)).replace('.','') + '_out' + str(np.round(p_out,3)).replace('.','') + '_p' + str(np.round(p,3)).replace('.','') + '_B' + str(B[b]).replace('.','')\n",
    "        file_name = 'correlation' + fixed + '_deg' + str(beta[b]) + '_' + design + '-full.csv'\n",
    "        \n",
    "        df = pd.read_csv(file_path + file_name)\n",
    "        \n",
    "        df = df.drop(df[((df['Estimator'] == 'LS-Prop') | (df['Estimator'] == 'LS-Num')) & (df['design'] == 'Cluster')].index)\n",
    "        df.loc[(df['Estimator']=='DM') & (df['design']=='Cluster'), 'Estimator'] = 'DM-C'\n",
    "        df.loc[(df['Estimator']=='DM($0.75$)') & (df['design']=='Cluster'), 'Estimator'] = 'DM-C($0.75$)'\n",
    "        \n",
    "        df.to_csv(file_path + file_name)\n",
    "        print(\"{} done\".format(file_name))\n",
    "\n",
    "\n",
    "'''\n",
    "file_name = 'correlation_n1000_nc50_in05_out00_p1_B002_deg1_bernoulli-full.csv'\n",
    "\n",
    "df = pd.read_csv(file_path + file_name)\n",
    "\n",
    "# delete rows where (Estimator == LS-Prop or Estimator == LS-Num) and design == Cluster\n",
    "df_new = df.drop(df[((df['Estimator'] == 'LS-Prop') | (df['Estimator'] == 'LS-Num')) & (df['design'] == 'Cluster')].index)\n",
    "\n",
    "# where Estimator == DM and Design == Cluster, rename Estimator to DM-C\n",
    "df_new.loc[(df['Estimator']=='DM') & (df['design']=='Cluster'), 'Estimator'] = 'DM-C'\n",
    "\n",
    "# where Estimator == DM($0.75$) and Design == Cluster, rename Esimator to DM-C($0.75$)\n",
    "df_new.loc[(df['Estimator']=='DM($0.75$)') & (df['design']=='Cluster'), 'Estimator'] = 'DM-C($0.75$)'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def outcome_sums(n, Y, Z, selected):\n",
    "    '''\n",
    "    Returns the sums of the outcomes Y(z_t) for each timestep t\n",
    "\n",
    "    Y (function): potential outcomes model\n",
    "    Z (numpy array): treatment vectors z_t for each timestep t\n",
    "    - each row should correspond to a timestep, i.e. Z should be beta+1 by n\n",
    "    selected (list): indices of units in the population selected to be part of the experiment (i.e in U)\n",
    "    '''\n",
    "    sums, sums_U = np.zeros(Z.shape[0]), np.zeros(Z.shape[0]) \n",
    "    if len(selected) == n: # if we selected all nodes, sums = sums_U\n",
    "        for t in range(Z.shape[0]):\n",
    "            outcomes = Y(Z[t,:])\n",
    "            sums[t] = np.sum(outcomes)\n",
    "            sums_U[t] = np.sum(outcomes)\n",
    "        return sums, sums_U\n",
    "    else: \n",
    "        for t in range(Z.shape[0]):\n",
    "            outcomes = Y(Z[t,:])\n",
    "            sums[t] = np.sum(outcomes)\n",
    "            sums_U[t] = np.sum(outcomes[selected])\n",
    "    return sums, sums_U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_array\n",
    "def binary_covariate_weights(n, nc, mu1, mu2, phi, A, seed=0):\n",
    "    ''' Returns weighted adjacency matrix, where weights depend on a binary covariate type\n",
    "\n",
    "    C[i,j] ~ Normal(mu1*mu1, 0.5) if both i and j are type 1\n",
    "    C[i,j] ~ Normal(mu2*mu2, 0.5) if both i and j are type 2\n",
    "    C[i,j] ~ Normal(mu1*mu2, 0.5) if i and j are different types\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nc : int\n",
    "        number of clusters\n",
    "    mu1 : float\n",
    "\n",
    "    mu2: float\n",
    "        \n",
    "    phi : float\n",
    "        probability of switching type\n",
    "    A : scipy sparse csr array\n",
    "        adjacency matrix\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    C : scipy sparse csr array\n",
    "        weighted adjacency matrix\n",
    "    '''\n",
    "    n = A.shape[0]\n",
    "    means = np.ones(n)\n",
    "    midpoint = int((nc//2)*(n/nc))\n",
    "    means[0:midpoint] = mu1\n",
    "    means[midpoint:] = mu2\n",
    "\n",
    "    rng  = np.random.default_rng(seed)\n",
    "    switchers = rng.random(n)\n",
    "    switchers = (switchers < phi) + 0 \n",
    "    means = np.concatenate((np.where(switchers[0:midpoint]==1, mu2, means[0:midpoint]), np.where(switchers[midpoint:]==1, mu1, means[midpoint:])))\n",
    "    means = np.outer(means, means)\n",
    "    weights = rng.normal(loc=means, scale=0.5)\n",
    "\n",
    "    return csr_array(A.multiply(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 1 1 0]\n",
      " [1 0 0 1]]\n",
      "\n",
      "[[-0.01783469  0.          1.90200002  0.        ]\n",
      " [ 0.         -0.38271074  0.          0.        ]\n",
      " [ 0.          1.14060417  5.62704453  0.        ]\n",
      " [ 0.97787051  0.          0.          6.77125668]]\n",
      "\n",
      "[-0.01783469 -0.38271074  1.14060417  0.97787051]\n",
      "\n",
      "  (0, 0)\t-0.01783468658055548\n",
      "  (1, 1)\t-0.38271073552302626\n",
      "  (2, 1)\t1.140604168033727\n",
      "  (3, 0)\t0.977870508571345\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "nc = 2\n",
    "mu1 = 1/2\n",
    "mu2  = 2.5\n",
    "phi = 0\n",
    "A = np.array([[1,0,1,0], [0,1,0,0], [0,1,1,0], [1,0,0,1]])\n",
    "A = csr_array(A)\n",
    "print(A.toarray())\n",
    "\n",
    "C = binary_covariate_weights(n, nc, mu1, mu2, phi, A)\n",
    "print()\n",
    "print(C.toarray())\n",
    "\n",
    "z = np.array([1,1,0,0])\n",
    "z_sp = csr_array(z)\n",
    "\n",
    "print()\n",
    "print(C.dot(z))\n",
    "\n",
    "print()\n",
    "print(C * z_sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Covariate Type (Binary) fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariate_type(i, n, K, num):\n",
    "    '''\n",
    "    Returns the covariate type of unit i. Assumes that the number of communities is divisible by the number of covariate types.\n",
    "\n",
    "    n (int) = number of units in the population\n",
    "    K (int) = number of communities (blocks) in the SBM\n",
    "    num (int)  = number of covariate types\n",
    "\n",
    "    Example: Suppose n=8, K=4, and num = 2. \n",
    "        Communities 0 and 1 are assigned covariate type 0 and contain individuals 0,1,2,3\n",
    "        Communities 2 and 3 are assigned covariate type 1 and contain individuals 4,5,6,7\n",
    "        Individual i's covariate type is i // 4. Note that 4 = n // num.\n",
    "\n",
    "    Example: Suppose n=8, K=2, and num = 2.\n",
    "        Community 0 is assigned covariate type 0 and contains individuals 0,1,2,3\n",
    "        Communities 1 is assigned covariate type 1 and contains individuals 4,5,6,7\n",
    "        Individual i's covariate type is i // 4. Note that 4 = n // num.\n",
    "\n",
    "    Example: Suppose n=8, K=4, and num = 4.\n",
    "        Community 0 is assigned covariate type 0 and contains individuals 0,1\n",
    "        Community 1 is assigned covariate type 1 and contains individuals 2,3\n",
    "        Community 2 is assigned covariate type 2 and contains individuals 4,5\n",
    "        Community 3 is assigned covariate type 3 and contains individuals 6,7\n",
    "        Individual i's covariate type is i // 2. Note that 2 = n // num.\n",
    "    '''\n",
    "    assert num <= K and num%K==0, \"there cannot be more covariate types than number of communities; number of types should divide evenly into the number of groups\"\n",
    "    div = n // num\n",
    "    return i // div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "K = 5\n",
    "num = 5\n",
    "\n",
    "for i in range(n):\n",
    "    print(covariate_type(i, n, K, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "C = np.zeros((5,5))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros(10)\n",
    "arr[0:10//2] = 1\n",
    "print(arr)\n",
    "arr[10//2:] = 2\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x2 = np.arange(3.0)\n",
    "print(x1)\n",
    "print()\n",
    "print(x2)\n",
    "print()\n",
    "print(np.multiply(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([0,0,0,0,1,1,1,1])\n",
    "P = np.random.rand(8)\n",
    "print(arr)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (P < 0.5)+0\n",
    "print(P)\n",
    "np.nonzero(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros(8) + 0.2\n",
    "print(arr)\n",
    "\n",
    "arr[np.nonzero(P)] = 0.8\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "sA = sparse.csr_matrix(arr)\n",
    "print(sA, sA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([2, 3, 1])\n",
    "arrB = np.multiply(arr,B)\n",
    "print(arrB, arrB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = sA.multiply(B)\n",
    "print(AB, AB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horvitz-Thompson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "nc = 50\n",
    "inds = [0, 10, 19, 20, 38, 39, 40, 59, 60]\n",
    "lst = len(set([i//(n/nc) for i in inds]))\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 8\n",
    "nc = 2\n",
    "# Replace this with your actual numpy array\n",
    "A = random_array = np.random.randint(2, size=(n, n))\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Get the indices of nonzero elements for each row\n",
    "nonzero_indices = [list(np.nonzero(row)[0]) for row in A]\n",
    "print(nonzero_indices)\n",
    "print()\n",
    "\n",
    "# list of cluster neighborhood sizes\n",
    "newList = [len(set([i // (n/nc) for i in neighbors])) for neighbors in nonzero_indices]\n",
    "print(newList)\n",
    "print()\n",
    "\n",
    "np.power(0.5, newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Replace this with your actual sparse matrix\n",
    "data = np.array([1, 0, 3, 0, 5, 0, 6, 0, 8])\n",
    "row_indices = np.array([0, 0, 0, 1, 1, 0, 2, 2, 2])\n",
    "col_indices = np.array([0, 2, 2, 0, 2, 0, 2, 2, 0])\n",
    "\n",
    "A = csr_matrix((data, (row_indices, col_indices)), shape=(3, 3))\n",
    "\n",
    "# Get the indices of nonzero elements for each row\n",
    "nonzero_indices = [list(row.nonzero()[1]) for row in A]\n",
    "\n",
    "print(nonzero_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Replace these with your actual arrays and dimensions\n",
    "n = 3  # Adjust the size of the arrays as needed\n",
    "data_z1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "z1 = csr_matrix(data_z1)\n",
    "z2 = np.array([10, 20, 30])\n",
    "\n",
    "# Find the nonzero indices for each row in z1\n",
    "neighborhoods = [list(row.nonzero()[1]) for row in A] # list of lists of nonzero indices in each row of A\n",
    "print(neighborhoods)\n",
    "print()\n",
    "\n",
    "neighbor_treatments = [list(z2[neighborhood]) for neighborhood in neighborhoods]\n",
    "print(neighbor_treatments)\n",
    "\n",
    "all_treated = [np.prod(treatments) for treatments in neighbor_treatments]\n",
    "none_treated = [all(z == 0 for z in treatments)+0 for treatments in neighbor_treatments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "# Replace this with your actual sparse matrix\n",
    "data = [[1, 0, 3], [0, 5, 0], [6, 0, 8]]\n",
    "sparse_matrix = scipy.sparse.csr_array(data)\n",
    "\n",
    "# Calculate the sum of each row\n",
    "row_sums = sparse_matrix.sum(axis=1).tolist()\n",
    "print(row_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def horvitz_thompson(n, nc, y, A, z, q, p):\n",
    "    '''Computes the Horvitz-Thompson estimate of the TTE under Bernoulli design or Cluster-Bernoulli design.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        the size of the population/network\n",
    "    nc : int\n",
    "        the number of clusters (equals n if simple Bernoulli design with no clustering)\n",
    "    y : numpy array\n",
    "        the outcomes of each unit in the population\n",
    "    A : scipy sparse array\n",
    "        adjacency matrix of the network such that A[i,j]=1 indicates that unit j is an in-neighbor of i\n",
    "    z : numpy array\n",
    "        the treatment assignment of each unit in the population\n",
    "    q : float\n",
    "        probability that a cluster is indepdently chosen for treatment (should equal 1 under simple Bernoulli design with no clustering)\n",
    "    p : float\n",
    "        the treatment probability for chosen clusters in the staggered rollout\n",
    "    '''\n",
    "\n",
    "    neighborhoods = [list(row.nonzero()[1]) for row in A] # list of neighbors of each unit\n",
    "    neighborhood_sizes = A.sum(axis=1).tolist() # size of each unit's neighborhood\n",
    "    neighbor_treatments = [list(z[neighborhood]) for neighborhood in neighborhoods] # list of treatment assignments in each neighborhood\n",
    "\n",
    "    A = A * csr_matrix(np.diag(np.repeat(np.arange(1,nc+1),n//nc))) # modifies the adjancecy matrix so that if there's an edge from j to i, A[i,j]=cluster(j)\n",
    "    cluster_neighborhoods = [np.unique(row.data,return_counts=True) for row in A] # for each i, cluster_neighborhoods[i] = [a list of clusters i's neighbors belong to, a list of how many neighbors are in each of these clusters]\n",
    "    cluster_neighborhood_sizes = [len(x[0]) for x in cluster_neighborhoods] # size of each unit's cluster neighborhood\n",
    "    \n",
    "    # Probabilities of each person's neighborhood being entirely treated or entirely untreated\n",
    "    all_treated_prob = np.multiply(np.power(p, neighborhood_sizes), np.power(q, cluster_neighborhood_sizes))\n",
    "    none_treated_prob = [np.prod((1+q) + np.power(1-p, x[1])*q) for x in cluster_neighborhoods]\n",
    "    \n",
    "    # Indicators of each person's neighborhood being entirely treated or entirely untreated\n",
    "    all_treated = [np.prod(treatments) for treatments in neighbor_treatments]\n",
    "    none_treated = [all(z == 0 for z in treatments)+0 for treatments in neighbor_treatments]\n",
    "\n",
    "    zz = np.nan_to_num(np.divide(all_treated,all_treated_prob) - np.divide(none_treated,none_treated_prob))\n",
    "\n",
    "    return 1/n * y.dot(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_functions import *\n",
    "import numpy as numpy\n",
    "def run_experiment(beta, n, nc, B, r, diag, Pii, Pij, phi, design, q_or_K, graphNum, T):\n",
    "    '''\n",
    "    beta = degree of the model / polynomial\n",
    "    n = population size\n",
    "    nc = number of clusters\n",
    "    B = original treatment budget/fraction\n",
    "    r = ratio offdiag/diag: (indirect effect)/(direct effects)\n",
    "    diag = maxium norm of the direct effects before covariate type scaling\n",
    "    Pii = edge probability within communities\n",
    "    Pij = edge prob btwn different communities\n",
    "    phi = correlation btwn community & effect type (probability between 0 and 0.5)\n",
    "    design = design being used for selecting clusters, either \"complete\" or \"bernoulli\"\n",
    "    q_or_K = if using complete RD for selecting cluster, this will be the value of K; if using Bernoulli design, this will be the value q\n",
    "    graphNum = number of graphs to average over\n",
    "    T = number of trials per graph\n",
    "    graphStr = type of graph e.g. \"SBM\" for stochastic block model or \"ER\" for Erdos-Renyi\n",
    "    p_prime = the budget on the boundary of U\n",
    "    '''\n",
    "    \n",
    "    offdiag = r*diag   # maximum norm of indirect effect\n",
    "\n",
    "    if design == \"complete\":\n",
    "        K = q_or_K\n",
    "        q = K/nc\n",
    "    else:\n",
    "        q = q_or_K\n",
    "        K = int(np.floor(q*nc))\n",
    "\n",
    "    p = B/q\n",
    "    G, A = SBM(n, nc, Pii, Pij)  #returns the SBM networkx graph object G and the corresponding adjacency matrix A\n",
    "\n",
    "    for g in range(graphNum):\n",
    "        # random weights for the graph edges\n",
    "        rand_wts = np.random.rand(n,3)\n",
    "        alpha = rand_wts[:,0].flatten()\n",
    "        C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "        C = covariate_weights_binary(C, minimal = 1/4, extreme = 4, phi=phi)\n",
    "        \n",
    "        # potential outcomes model\n",
    "        fy = ppom(beta, C, alpha)\n",
    "\n",
    "        # compute the true TTE\n",
    "        TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "        print(\"TTE: {}\\n\".format(TTE))\n",
    "        \n",
    "        ####### Estimate ########\n",
    "\n",
    "        # parameters for the staggered rollout - Cluster Randomized Design\n",
    "        P = seq_treatment_probs(beta, p)        # treatment probabilities for each step of the staggered rollout on U\n",
    "        P_prime = seq_treatment_probs(beta, 0)  # treatment probabilities for each step of the staggered rollout on the boundary of U\n",
    "\n",
    "        TTE_ht = np.zeros(T)\n",
    "        for i in range(T):\n",
    "            # select clusters \n",
    "            if design == \"complete\":\n",
    "                selected = select_clusters_complete(nc, K)\n",
    "            else:\n",
    "                selected = select_clusters_bernoulli(nc, q)\n",
    "            \n",
    "            # U\n",
    "            selected_nodes = [x for x,y in G.nodes(data=True) if (y['block'] in selected)] # get the nodes in selected clusters\n",
    "\n",
    "            # Cluster Randomized Design\n",
    "            Z = staggered_rollout_bern_clusters(n, selected_nodes, P, [], P_prime)\n",
    "            z = Z[beta,:]\n",
    "            y = fy(z)\n",
    "\n",
    "            TTE_ht[i] = horvitz_thompson(n, nc, y, A, z, q, p)\n",
    "\n",
    "        print(\"H-T: {}\".format(np.sum(TTE_ht)/T))\n",
    "        print(\"H-T MSE: {}\\n\".format(np.sum((TTE_ht-TTE)**2)/T))\n",
    "\n",
    "def covariate_weights_binary(C, minimal = 1/4, extreme = 4, phi=0):\n",
    "    '''\n",
    "    Returns a weighted adjacency matrix where weights are determined by covariate type. We assume a binary effect types covariate\n",
    "\n",
    "    C (array): weights without effect type covariate\n",
    "    minimal (float): \n",
    "    extreme (int):\n",
    "    phi: probability that an individual's covariate type flips\n",
    "    '''\n",
    "    n = C.shape[0]\n",
    "    scaling1 = np.zeros(n//2) + minimal\n",
    "    scaling2 = np.zeros(n//2) + extreme\n",
    "\n",
    "    R1 = np.random.rand(n//2)\n",
    "    R2 = np.random.rand(n//2)\n",
    "    R1 = (R1 < phi) + 0\n",
    "    R2 = (R2 < phi) + 0\n",
    "\n",
    "    scaling1[np.nonzero(R1)] = extreme\n",
    "    scaling2[np.nonzero(R2)] = minimal\n",
    "\n",
    "    scaling = np.concatenate((scaling1,scaling2))\n",
    "    return C.multiply(scaling)\n",
    "\n",
    "def SBM(n, k, Pii, Pij):\n",
    "    '''\n",
    "    Returns the adjacency matrix of a stochastic block model on n nodes with k communities\n",
    "    The edge prob within the same community is Pii\n",
    "    The edge prob across different communities is Pij\n",
    "    '''\n",
    "    sizes = np.zeros(k, dtype=int) + n//k\n",
    "    probs = np.zeros((k,k)) + Pij\n",
    "    np.fill_diagonal(probs, Pii)\n",
    "    G = nx.stochastic_block_model(sizes, probs)\n",
    "    A = nx.adjacency_matrix(nx.stochastic_block_model(sizes, probs))\n",
    "    #blocks = nx.get_node_attributes(G, \"block\")\n",
    "    return G, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTE: 2.528946140581735\n",
      "\n",
      "H-T: -0.33747227618894143\n",
      "H-T MSE: 8.216938497017704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta = 1\n",
    "n = 1000\n",
    "nc = 50\n",
    "B = 0.06\n",
    "p = 1\n",
    "r = 1.25\n",
    "diag = 1\n",
    "Pii = 10/(n/nc)\n",
    "Pij = 0\n",
    "phi = 0\n",
    "#design = \"complete\"\n",
    "#q_or_K = int(np.floor(B * nc / p))\n",
    "design = \"bernoulli\"\n",
    "q_or_K = 0.6\n",
    "graphNum = 1\n",
    "T = 100\n",
    "\n",
    "run_experiment(beta, n, nc, B, r, diag, Pii, Pij, phi, design, q_or_K, graphNum, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_treated = [0, 1, 0, 0, 0]\n",
    "all_treated_prob = [0, 0.25, 0.75, 0.25, 0]\n",
    "none_treated = [0, 0, 1, 0, 1]\n",
    "none_treated_prob = [0, 0.25, 0.25, 0.5, 0.75]\n",
    "arr1 = np.divide(all_treated,all_treated_prob)\n",
    "arr2 = np.divide(none_treated,none_treated_prob)\n",
    "arr3 = arr1 - arr2\n",
    "print(arr1)\n",
    "print()\n",
    "print(arr2)\n",
    "print()\n",
    "print(arr3)\n",
    "np.nan_to_num(arr1, copy=False)\n",
    "np.nan_to_num(arr2, copy = False)\n",
    "print(arr1)\n",
    "print()\n",
    "print(arr2)\n",
    "print()\n",
    "np.nan_to_num(arr3, copy = False)\n",
    "print(arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "nc = 50\n",
    "l = 1\n",
    "\n",
    "cluster = set(range(l*(n//nc), (l+1)*(n//nc)))\n",
    "neighbors = set([20, 25, 40, 62])\n",
    "\n",
    "len(cluster & neighbors)\n",
    "\n",
    "for c in cluster:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import random, csr_matrix\n",
    "\n",
    "n = 10\n",
    "nc = 5\n",
    "\n",
    "random_sparse_matrix = random(n, n, density=0.5, format='csr')\n",
    "random_sparse_matrix.data[:] = 1\n",
    "print(random_sparse_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 0 0]\n",
      " [0 0 0 0 0 0 0 4 0 0]\n",
      " [0 0 0 0 0 0 0 0 5 0]\n",
      " [0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      "[[1. 1. 0. 2. 3. 3. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 2. 3. 3. 4. 4. 0. 0.]\n",
      " [1. 1. 2. 0. 3. 3. 0. 4. 5. 5.]\n",
      " [0. 0. 2. 0. 3. 0. 0. 0. 5. 0.]\n",
      " [0. 0. 2. 2. 0. 3. 0. 4. 5. 0.]\n",
      " [1. 1. 2. 0. 0. 3. 4. 0. 5. 0.]\n",
      " [0. 0. 0. 0. 0. 3. 0. 4. 0. 5.]\n",
      " [1. 0. 0. 0. 3. 3. 0. 0. 5. 5.]\n",
      " [1. 0. 0. 2. 0. 0. 0. 0. 5. 0.]\n",
      " [1. 1. 0. 2. 0. 3. 4. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sp = csr_matrix(np.diag(np.repeat(np.arange(1,nc+1),n//nc)))\n",
    "print(sp.toarray())\n",
    "\n",
    "#clusters = np.arange(1,nc+1)\n",
    "#clusters = np.repeat(clusters,n//nc)\n",
    "#clusters = np.tile(np.repeat(np.arange(1,nc+1),n//nc), (n,1))\n",
    "\n",
    "#myprod = random_sparse_matrix.multiply(np.tile(np.repeat(np.arange(1,nc+1),n//nc), (n,1)))\n",
    "myprod2 = random_sparse_matrix * sp\n",
    "\n",
    "print()\n",
    "print(myprod2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(random_sparse_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 1]\n",
      " [0 0 0 0 0 1 0 1 1 1]\n",
      " [0 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]] <class 'scipy.sparse._arrays.csr_array'>\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_array\n",
    "G,A = SBM(10, 2, 2/(10/2), 0)\n",
    "print(A.toarray(), type(A))\n",
    "\n",
    "temp = csr_array(np.tile(np.repeat(np.arange(1,nc+1),n//nc), (n,1)))#csr_array(np.diag(np.repeat(np.arange(1,nc+1),n//nc)))\n",
    "print(temp.toarray())\n",
    "\n",
    "my = A.multiply(temp) #A * temp\n",
    "print(my.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 1 1]\n",
      " [0 0 0 1 0]\n",
      " [1 0 1 0 1]]\n",
      "\n",
      "[0 0 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.prod(A*z,axis=1)\n",
    "\n",
    "A = np.array([[1,0,0,1,1], [0,1,0,1,0], [0,0,1,1,1], [0,0,0,1,0], [1,0,1,0,1]])\n",
    "z = np.array([0,0,1,1,1])\n",
    "print(A)\n",
    "print()\n",
    "print(z)\n",
    "A&z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 1 0 1]\n",
      " [0 0 0 0 0 0 1 1 1 0]]\n",
      "\n",
      "[[0 0 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 2]\n",
      " [0 0 0 0 0 2 0 0 2 2]\n",
      " [0 0 0 0 0 0 2 2 0 2]\n",
      " [0 0 0 0 0 0 2 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "nc = 2\n",
    "avg = 2\n",
    "G,A = SBM(n, nc, avg/(n/nc), 0)\n",
    "z = np.repeat([1, 0], 5)\n",
    "print(A.toarray())\n",
    "print()\n",
    "\n",
    "neighborhoods = [list(row.nonzero()[1]) for row in A] # list of neighbors of each unit\n",
    "neighborhood_sizes = A.sum(axis=1).tolist() # size of each unit's neighborhood\n",
    "neighbor_treatments = [list(z[neighborhood]) for neighborhood in neighborhoods] # list of treatment assignments in each neighborhood\n",
    "\n",
    "A = A.multiply(csr_array(np.tile(np.repeat(np.arange(1,nc+1),n//nc), (n,1)))) # modifies the adjancecy matrix so that if there's an edge from j to i, A[i,j]=cluster(j)\n",
    "print(A.toarray())\n",
    "cluster_neighborhoods = [np.unique(row.data,return_counts=True) for row in A] # for each i, cluster_neighborhoods[i] = [a list of clusters i's neighbors belong to, a list of how many neighbors are in each of these clusters]\n",
    "cluster_neighborhood_sizes = [len(x[0]) for x in cluster_neighborhoods] # size of each unit's cluster neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [3]\n",
      "[1] [1]\n",
      "[1] [2]\n",
      "[1] [1]\n",
      "[1] [3]\n",
      "[2] [1]\n",
      "[2] [2]\n",
      "[2] [3]\n",
      "[2] [3]\n",
      "[2] [3]\n"
     ]
    }
   ],
   "source": [
    "for x in cluster_neighborhoods:\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_neighborhood_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "datatypes = {'Estimator': str, 'Bias': np.float64, 'Abs_Bias': np.float64, 'Rel_bias_sq': np.float64, 'Bias_sq': np.float64}\n",
    "df = pd.read_csv('output/deg1/SBM_n1000_nc50_in04_out0002_p1_B006_correlation-full-data_deg1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'n', 'nc', 'Pii', 'Pij', 'Phi', 'K', 'p', 'q', 'B',\n",
       "       'ratio', 'out-in', 'Graph', 'rep', 'design', 'Estimator', 'Bias',\n",
       "       'Abs_Bias', 'Rel_bias_sq', 'Bias_sq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'a': np.float64, 'b': np.int32, 'c': 'Int64'} str \n",
    "#Bias,Abs_Bias,Rel_bias_sq,Bias_sq\n",
    "\n",
    "datatypes = {'Estimator': str, 'Bias': np.float64, 'Abs_Bias': np.float64, 'Rel_bias_sq': np.float64, 'Bias_sq': np.float64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   PI-$n$($p$)\n",
       "1         PI-$\\mathcal{U}$($p$)\n",
       "2                            HT\n",
       "3                       LS-Prop\n",
       "4                        LS-Num\n",
       "                  ...          \n",
       "118795              PI-$n$($B$)\n",
       "118796                  LS-Prop\n",
       "118797                   LS-Num\n",
       "118798                       DM\n",
       "118799               DM($0.75$)\n",
       "Name: Estimator, Length: 118800, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'Estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance([5.234567], list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
